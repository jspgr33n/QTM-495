#' Generates image and video representations useful in earth observation tasks for casual inference.
#'
#' Generates image and video representations useful in earth observation tasks for casual inference, following the approach in Rolf, Esther, et al.  (2021).
#'
#' @param file Path to a tfrecord file generated by `causalimages::WriteTfRecord`.
#' @param imageKeysOfUnits A vector of length `length(imageKeysOfUnits)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into the package to call images into memory.
#' @param conda_env A `conda` environment where computational environment lives, usually created via `causalimages::BuildBackend()`. Default = `"CausalImagesEnv"`
#' @param conda_env_required A Boolean stating whether use of the specified conda environment is required.
#' @param kernelSize Dimensions used in the convolution kernels.
#' @param nWidth_ImageRep Number of embedding features output.
#' @param strides  Integer specifying the strides used in the convolutional layers.
#' @param InitImageProcess (default = `NULL`) Initial image processing function. Usually left `NULL`.
#' @param batchSize Integer specifying batch size in obtaining representations.
#' @param dataType String specifying whether to assume `"image"` or `"video"` data types. Default is `"image"`.
#' @param TfRecords_BufferScaler The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#' @param seed Integer specifying the seed for pseudo random number generation.
#'
#' @return A list containing two items:
#' \itemize{
#' \item `Representations` (matrix) A matrix containing image/video representations, with rows corresponding to observations.
#' \item `ImageRepArm_OneObs,ImageRepArm_batch_R, ImageRepArm_batch` (functions) Image modeling functions.
#' \item `ImageModel_And_State_And_MPPolicy_List` List containing image model parameters fed into functions.
#' }
#'
#' @section References:
#' \itemize{
#' \item Rolf, Esther, et al. "A generalizable and accessible approach to machine learning with global satellite imagery." *Nature Communications* 12.1 (2021): 4392.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md

GetImageRepresentations <- function(
    imageKeysOfUnits = NULL,
    file = NULL,
    conda_env = "CausalImagesEnv",
    conda_env_required = T,
    returnContents = T,
    getRepresentations = T,
    imageModelClass = "VisionTransformer",
    NORM_MEAN = NULL, 
    NORM_SD = NULL, 
    Sys.setenv_text = NULL,

    InitImageProcess = NULL,
    pretrainedModel = NULL, 
    lat = NULL, long = NULL, 
    image_dtype = NULL, 
    image_dtype_tf = NULL,  
    nWidth_ImageRep = 64L,
    nDepth_ImageRep = 1L,
    nDepth_TemporalRep = 1L,
    batchSize = 16L,
    optimizeImageRep = T,
    strides = 1L,
    kernelSize = 3L,
    patchEmbedDim = 16L,
    TfRecords_BufferScaler = 10L,
    dropoutRate,
    dataType = "image",
    bn_momentum = 0.99,
    inputAvePoolingSize = 1L, # set > 1L if seeking to downshift the image resolution
    CleanupEnv = FALSE, 
    initializingFxns = FALSE, 
    seed = NULL){

  # initialize tensorflow if not already initialized
  if(   !"logical" %in% class(try(as.numeric(np$array(jnp$square(1.)))==1,T)) ){
    print2("Establishing connection to computational environment (build via causalimages::BuildBackend())")
    library( tensorflow ); 
    if(!is.null(conda_env)){ try(reticulate::use_condaenv(conda_env, required = conda_env_required),T) }
    if(!is.null(Sys.setenv_text)){ 
      eval(parse(text = Sys.setenv_text), envir = .GlobalEnv)
    }
    py_gc <<- reticulate::import("gc")
    (jax <<- reticulate::import("jax"))$config$update("jax_enable_x64", FALSE);
    jnp <<- reticulate::import("jax.numpy")
    np <<- reticulate::import("numpy")
    jmp <<- reticulate::import("jmp")
    optax <<- reticulate::import("optax")
    eq <<- reticulate::import("equinox")
    
    # set memory growth for tensorflow 
    for(device_ in tf$config$list_physical_devices()){ try(tf$config$experimental$set_memory_growth(device_, T),T) }
  }
  gc(); try(py_gc$collect(), T)
  if(is.null(seed)){ seed <- as.integer(runif(1,1,10000)) }

  # image dtype
  if(is.null(image_dtype)){
    image_dtype <- jnp$float16
    image_dtype_tf <- tf$float16
  }
  NeuralizeScale <- FALSE; nScalePatches <- ai(3L^2)
  
  print2("Setting input types ...") 
  if(!is.null(pretrainedModel)){ pretrainedModel <- as.character(pretrainedModel) } 
  if(!is.null(optimizeImageRep)){ optimizeImageRep <- as.logical(optimizeImageRep) }
  if(!is.null(imageModelClass)){ imageModelClass <- as.character(imageModelClass) }
  if(!is.null(nWidth_ImageRep)){ nWidth_ImageRep <- as.integer(f2n(nWidth_ImageRep)) }

  if( batchSize > length( unique(imageKeysOfUnits) )){
    batchSize <- length( unique(imageKeysOfUnits)  )
  }

  # define base tf record + train/test fxns
  orig_wd <- getwd()
  if(  !is.null(  file  )  ){
    # established tfrecord connection
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    setwd( new_wd <- paste(tf_record_name[-length(tf_record_name)],collapse = "/") )
    tf_dataset = tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideo <- (dataType == "video")
    getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      return( dataset <- dataset$batch( as.integer(max(2L, round(batchSize/2L)  ))) )
    }

    getParsed_tf_dataset_train <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      dataset <- dataset$shuffle(tf$constant(as.integer(TfRecords_BufferScaler*batchSize), dtype=tf$int64),
                                 reshuffle_each_iteration = T)
      dataset <- dataset$batch(as.integer(batchSize))
    }

    # setup iterators
    tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )

    # reset iterators
    ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )
  }

  # acquire image to set dimensions
  setwd(orig_wd); test_ <- tf$expand_dims(GetElementFromTfRecordAtIndices( uniqueKeyIndices = 1L,
                                                             filename = file,
                                                             readVideo = useVideo,
                                                             image_dtype = image_dtype_tf,
                                                             nObs = length(unique(imageKeysOfUnits)))[[1]],0L); setwd(new_wd)
  imageDims <- ai( length( dim(test_) ) - 2L )
  rawChannelDims <- ai( dim(test_)[length(dim(test_))] )
  rawSpatialDims <- ai( dim(test_)[length(dim(test_))-1] )
  
  if(grepl(pretrainedModel,pattern="clay")){ nWidth_ImageRep <- nWidth_VideoRep <- 768L }
  if(grepl(pretrainedModel,pattern="clip")){ nWidth_ImageRep <- nWidth_VideoRep <- 512L }
  if(grepl(pretrainedModel,pattern="swin")){ nWidth_ImageRep <- nWidth_VideoRep <- 768L }

  # setup jax model
  {
    print2("Setting up image representation model...")
    if(dataType == "video" | NeuralizeScale){ NonLinearScaler <- jnp$array( 1/sqrt( 2*(nDepth_ImageRep + nDepth_TemporalRep )) )  };
    if(dataType == "image"){ NonLinearScaler <- jnp$array( 1/sqrt( 2*nDepth_ImageRep ) )  }
    MPList <- list(jmp$Policy(compute_dtype="float16",  param_dtype="float32", output_dtype="float32"),
                   jmp$DynamicLossScale(jnp$array(2^15), period = 1000L))

    # coerce to integer for safety
    kernelSize <- ai(kernelSize); strides <- ai(strides)
    rawChannelDims <- ai(rawChannelDims); nWidth_ImageRep <- ai(nWidth_ImageRep)

    # set batch name
    batch_axis_name <- "batch";
    if(!"bn_momentum" %in% ls()){ bn_momentum <- 0.90 }

    # transformer preliminaries
    ffmap <- jax$vmap(function(L_, x){ L_(x) }, in_axes = list(NULL,0L))
    RMS_norm <- function(x_){ jnp$divide( x_, jnp$sqrt(0.001+jnp$mean(jnp$square(x_), -1L, keepdims=T))) }
    LayerNorm <- function(x_){ jax$nn$standardize( x_, -1L, epsilon = 1e-5) }
    InvSoftPlus <- function(x){ jnp$log(jnp$exp(x) - 1) }
    
    # Calculate number of patches
    nPatches_side = ai(rawSpatialDims / patchEmbedDim)
    nPatches = ai(nPatches_side^2)
    InitialPatchDims <- ai( (nPatches_side*patchEmbedDim*nPatches_side*patchEmbedDim)/nPatches * rawChannelDims )
    
    # adjust widths as needed 
    nWidth_VideoRep <- ifelse(optimizeImageRep,
                              yes = nWidth_ImageRep,
                              no = nWidth_ImageRep + 2L*nWidth_ImageRep*(imageModelClass=="CNN" & 
                                                                           optimizeImageRep == F ))
    if(!is.null(pretrainedModel)){ if(grepl(pretrainedModel, pattern = "videomae")){ nWidth_ImageRep <- nWidth_VideoRep <- 2L*768L } }  
    
    # rotary embedding setup
    RotaryPositionalEmbeddings_spatial <- eq$nn$RotaryPositionalEmbedding( ifelse(optimizeImageRep, 
                                                                          yes = nWidth_ImageRep, 
                                                                          no = ifelse(imageModelClass == "CNN" & is.null(pretrainedModel), 
                                                                                      yes = 3L*nWidth_ImageRep, no = nWidth_ImageRep) ) ) 
    RotaryPositionalEmbeddings_temporal <- eq$nn$RotaryPositionalEmbedding( nWidth_VideoRep  ) 
    WideMultiplicationFactor <- 3.5
    nTransformerOutputWidth <- nWidth_ImageRep
    
    # setup initial processor 
    if( is.null(InitImageProcess) ){
      InitImageProcess <- (function(im, seed, inference =  F){ return( im  ) })
    }
    if( !is.null(pretrainedModel) ){
      InitImageProcess_orig <- InitImageProcess
      InitImageProcess <- function(m, seed, inference){ 
        # normalize for this/these models
        if( grepl(pretrainedModel,pattern="clay")  ){ 
          m <- InitImageProcess_orig(m, jax$random$PRNGKey(1L), T) 
        }
        
        # model specific transformations 
        if(!grepl(pretrainedModel,pattern="video")){
          if( dataType == "video" ){ 
            m_shape_orig <- m$shape
            m <- jnp$reshape(m, list(-1L,m$shape[[3]],m$shape[[4]], m$shape[[5]]))
          }
          if(m$shape[[4]] == 1L){ m <- m * jnp$expand_dims(jnp$expand_dims(jnp$array(t(c(1,1,1))),0L),0L)$astype(m$dtype) }
          if(m$shape[[4]] > 3L){ m <- jnp$take(m,0L:2L,axis=3L) }
        }
        if( pretrainedModel == "clip-rsicd" ){
          # https://huggingface.co/flax-community/clip-rsicd-v2
          if(!"FeatureExtractor" %in% ls(.GlobalEnv)){
            print("Loading pre-trained model (clip-rsicd)...")
            
            TransformersModule <<- reticulate::import("transformers") 
            torch <<- reticulate::import("torch") 
            
            PretrainedImageModelName <<- "flax-community/clip-rsicd-v2"
            FeatureExtractor <<- TransformersModule$CLIPProcessor$from_pretrained(PretrainedImageModelName)
            torch$set_default_device(
              RunOnDevice <<- ifelse(torch$cuda$is_available(), 
                                     yes = list(torch$device("cuda")), 
                                     no = list(torch$device("cpu")))[[1]] 
            )
            torch$set_default_dtype( RunDtype <<- torch$float32 ); 
            TransformersModel <<- TransformersModule$CLIPModel$from_pretrained(PretrainedImageModelName)$to(RunOnDevice)#$half()
            TransformersProcessor <<- TransformersModule$CLIPImageProcessor$from_pretrained(PretrainedImageModelName)
            nParameters_Pretrained <<- TransformersModel$num_parameters()

            # from examining FeatureExtractor (?)
            MEAN_RESCALER <<- jnp$array(c(0, 0,0))
            MEAN_RESCALER <<- jnp$reshape(MEAN_RESCALER,list(1L,3L,1L,1L))
            
            SD_RESCALER <<- jnp$array(c(1,1,1))
            SD_RESCALER <<- jnp$reshape(SD_RESCALER,list(1L,3L,1L,1L))
            
            NORM_MEAN_array_inner <<- jnp$reshape(jnp$array(NORM_MEAN),list(1L,1L,1L,3L))
            NORM_SD_array_inner <<- jnp$reshape(jnp$array(NORM_SD),list(1L,1L,1L,3L))
            
            ScaleResizeTranspose <<- jax$jit(function(m){
              # m <- (m - NORM_MEAN_array_inner) / NORM_SD_array_inner
              m <- jnp$clip((m / 255.) ,0.0001,0.9999)
              m <- jax$image$resize( image=m,
                                     shape=c(m$shape[[1]], 224L, 224L, 3L), method="bilinear")
              m <- jnp$transpose(m, c(0L,3L,1L,2L))
            })
          }
          
          # stretch and re-normalize. see FeatureExtractor for details 
          m <- ScaleResizeTranspose(m)
          # m <- (m*SD_RESCALER)+MEAN_RESCALER
          # jnp$mean(jnp$array(m), axis = c(0L,2L:3L)); jnp$std(jnp$array(m), axis =  c(0L,2L:3L)) 
          m <- torch$tensor( reticulate::np_array( tf$constant(m, tf$float32), dtype = np$float32), 
                             dtype = torch$float32)
          m <- TransformersProcessor(m, do_rescale = F, return_tensors="pt")['pixel_values']
          # m <- reticulate::np_array( tf$constant(m, tf$float32), dtype = np$float32)
          # m <- FeatureExtractor(images = m, return_tensors="pt", do_resize = T)["pixel_values"]$type(RunDtype)$to(RunOnDevice)
          m <- TransformersModel$get_image_features(pixel_values = m)$cpu()$detach()$numpy() 
          # plot(m[,1:10])
          py_gc$collect()
        }
        if(  grepl(pretrainedModel, pattern = "swin") ){
          if(!"FeatureExtractor" %in% ls(.GlobalEnv)){
            print2("Setting up swin model...")
            TransformersModule <<- import("transformers")
            nParameters_Pretrained <<- 1L
            
            # didn't get working (uses haiku)
            #FeatureExtractor <<- (jax_models <<- import("jax_models",convert = T))$load_model('swin-tiny-224', num_classes=1000L, attach_head=FALSE, pretrained=TRUE)
            #Processor <<- TransformersModule$Swin2SRImageProcessor('swin-tiny-224')
            
            torch <<- import("torch")
            torch$set_default_device( RunOnDevice <<- ifelse(torch$cuda$is_available(), 
                                     yes = list(torch$device("cuda")),  no = list(torch$device("cpu")))[[1]]  )
            torch$set_default_dtype( RunDtype <<- torch$float32 ); 
            FeatureExtractor <<- TransformersModule$Swinv2Model$from_pretrained(PretrainedImageModelName <<- "microsoft/swinv2-tiny-patch4-window8-256")$to(RunOnDevice)
            Processor <<- TransformersModule$AutoImageProcessor$from_pretrained(PretrainedImageModelName)
              
            # got working (uses flax)
            #jax_models$list_models()
            #FeatureExtractor <<- TransformersModule$FlaxCLIPModel$from_pretrained(PretrainedImageModelName <<- "flax-community/clip-rsicd-v2")
            #Processor <<- TransformersModule$CLIPProcessor$from_pretrained(PretrainedImageModelName)
            #FeatureExtractor <<- TransformersModule$FlaxCLIPModel$from_pretrained(PretrainedImageModelName <<- "openai/clip-vit-base-patch32")
            
            #nWidth_ImageRep <<- nWidth_VideoRep <<- 768L
            #FeatureExtractor <<- TransformersModule$FlaxViTModel$from_pretrained(PretrainedImageModelName <<- "facebook/vit-mae-base")

            # stretch and renormalization setups 
            MEAN_RESCALER <<- jnp$array(c(0,0,0))
            MEAN_RESCALER <<- jnp$reshape(MEAN_RESCALER,list(1L,3L,1L,1L))
            
            SD_RESCALER <<- jnp$array(c(1,1,1))
            SD_RESCALER <<- jnp$reshape(SD_RESCALER,list(1L,3L,1L,1L))
            
            NORM_MEAN_array_inner <<- jnp$reshape(jnp$array(NORM_MEAN),list(1L,1L,1L,3L))
            NORM_SD_array_inner <<- jnp$reshape(jnp$array(NORM_SD),list(1L,1L,1L,3L))
            print2("Done setting up swin model!")
          }
          
          # normalize and resize 
          #m <- (m - NORM_MEAN_array_inner) / NORM_SD_array_inner
          if(!NeuralizeScale){
            m <- jax$image$resize( image=m, shape=c(m$shape[[1]], 224L, 224L, 3L), method="bilinear")
          }
          m <- jnp$transpose(m, c(0L,3L,1L,2L))
          #m <- (m*SD_RESCALER)+MEAN_RESCALER
          
          #m <- 1 * (m - m$min(axis = 1L:3L,keepdims=T)) /  (m$max(axis = 1L:3L,keepdims=T) - m$min(axis = 1L:3L,keepdims=T))
          m <- 1 * (m - m$min()) /  (m$max() - m$min())
          m <-  torch$tensor( reticulate::np_array( tf$constant(m, tf$float32), dtype = np$float32), dtype = torch$float32)
          m <- Processor(images = m, return_tensors="pt", do_rescale = F)$data$pixel_values # jax if using flax 
          
          #tmp <- FeatureExtractor$get_image_features(pixel_values =  jnp$transpose(jax$image$resize( image=m,shape=c(m$shape[[1]], 224L, 224L, 3L), method="bilinear"), c(0L,3L,1L,2L)), params = FeatureExtractor$params, train = !inference )
          #tmp1 <- FeatureExtractor$get_image_features(pixel_values = m, params = FeatureExtractor$params, train = !inference )
          #hist(np$array(tmp)); hist(np$array(tmp1))
          #plot(c(np$array(tmp)),np$array(tmp1)); abline(a=0,b=1)
          
          if(pretrainedModel != "swin-ft"){ 
            m <- jnp$array(FeatureExtractor( torch$as_tensor( m, dtype= torch$float32) )$pooler_output$detach()$numpy())
            #m <- FeatureExtractor$get_image_features(pixel_values = m, params = FeatureExtractor$params,dropout_rng = seed,train = !inference) # for flax models  
            #m <- FeatureExtractor$get_image_features(pixel_values = m, params = FeatureExtractor$params,dropout_rng = seed,train = !inference))$pooler_output # for flax models  (vae)
          }
          py_gc$collect()
        }
        if( grepl(pretrainedModel, pattern = "vit-base") ){ 
          if(!"FeatureExtractor" %in% ls(.GlobalEnv)){
            print("Loading pre-trained model (vit-base)...")
            TransformersModule <<- reticulate::import("transformers") 
            torch <<- reticulate::import("torch") 
            PretrainedImageModelName <<- 'google/vit-base-patch16-224-in21k'
            FeatureExtractor <<- TransformersModule$ViTImageProcessor$from_pretrained(PretrainedImageModelName)
            torch$set_default_device(
              RunOnDevice <<- ifelse(torch$cuda$is_available(), 
                                     yes = list(torch$device("cuda")), 
                                     no = list(torch$device("cpu")))[[1]] 
            )
            torch$set_default_dtype( RunDtype <<- torch$float32 ); 
            TransformersModel <<- TransformersModule$ViTModel$from_pretrained(PretrainedImageModelName)$to(RunOnDevice)#$half()
            nParameters_Pretrained <<- TransformersModel$num_parameters()
            nWidth_ImageRep <<- nWidth_VideoRep <<- 768L
            
            MEAN_RESCALER <<- jnp$array(c(0.5,0.5,0.5))
            MEAN_RESCALER <<- jnp$reshape(MEAN_RESCALER,list(1L,3L,1L,1L))
            
            SD_RESCALER <<- jnp$array(c(0.5,0.5,0.5))
            SD_RESCALER <<- jnp$reshape(SD_RESCALER,list(1L,3L,1L,1L))
            
            NORM_MEAN_array_inner <<- jnp$reshape(jnp$array(NORM_MEAN),list(1L,1L,1L,3L))
            NORM_SD_array_inner <<- jnp$reshape(jnp$array(NORM_SD),list(1L,1L,1L,3L))
          }
          
          # Images are resized/rescaled to the same resolution (224x224)
          # and normalized across the RGB channels with mean (0.5, 0.5, 0.5) 
          # and standard deviation (0.5, 0.5, 0.5)
          # evidence: https://huggingface.co/google/vit-base-patch16-224-in21k  
          # (see preprocessing)
          # m_orig <- m
          # jnp$mean(jnp$array(m), axis = c(0L:2L)); jnp$std(jnp$array(m), axis =  c(0L:2L)) 
          m <- (m - NORM_MEAN_array_inner) / NORM_SD_array_inner
          
          # resize information: 
          #the output shape, as a sequence of integers with length equal to
          #the number of dimensions of `image`. Note that :func:`resize` does not
          #distinguish spatial dimensions from batch or channel dimensions, so this
          #includes all dimensions of the image. To represent a batch or a channel
          #dimension, simply leave that element of the shape unchanged.
          
          # sanity checks 
          #image2(np$array(m[1,,,1]))
          #image2(np$array(jax$image$resize( image=m, shape=c(m$shape[[1]], 224L, 224L, 3L), method="bilinear")[1,,,1]))
          #image2(np$array(jax$image$resize( image=m, shape=c(m$shape[[1]], 64L, 64L, 3L), method="bilinear")[1,,,1]))
          #image2(np$array(jax$image$resize( image=m, shape=c(m$shape[[1]], 32L, 32L, 3L), method="bilinear")[1,,,1]))
          
          # run resizing 
          m <- jax$image$resize(
            image=m,
            shape=c(m$shape[[1]], 224L, 224L, 3L),
            method="bilinear")
          
          #m <- FeatureExtractor(images = m, return_tensors="pt",do_resize = T, do_rescale = F, do_normalize = F)["pixel_values"]$type(RunDtype)$to(RunOnDevice)
          m <- jnp$transpose(m, c(0L,3L,1L,2L))
          m <- (m*SD_RESCALER)+MEAN_RESCALER
          # jnp$mean(jnp$array(m), axis = c(0L,2L:3L)); jnp$std(jnp$array(m), axis =  c(0L,2L:3L)) 
          m <- torch$tensor( reticulate::np_array( tf$constant(m, tf$float32), dtype = np$float32), dtype = torch$float32)
          m <- TransformersModel(m)$pooler_output$cpu()$detach()$numpy() 
          py_gc$collect()
        }
        if( grepl(pretrainedModel, pattern = "clay") ){ 
          # https://clay-foundation.github.io/model/tutorials/clay-v1-wall-to-wall.html
          #Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
          if(!"ClayModel" %in% ls(.GlobalEnv)){
            print("Loading pre-trained model (Clay)...")
            torch <<- import("torch")
            if( !Sys.info()["machine"] == "x86_64" ){ 
              oldwd <- getwd(); setwd("~/Documents/model/");
              ClayModel <<- reticulate::import_from_path("model", path = "./src")
              setwd(oldwd)
              ClayModel <<- ClayModel$ClayMAEModule$load_from_checkpoint(
                "/Users/cjerzak/Documents/Clay/Clay-1.0.5.7_epoch-13_val-loss-0.3098.ckpt", 
                metadata_path="/Users/cjerzak/Documents/model/configs/metadata.yaml", 
                shuffle=F,  mask_ratio=0, batch_first = T)
            }
            if( Sys.info()["machine"] == "x86_64" & T == F ){ 
              ClayModel <<- ClayModel$ClayMAEModule$load_from_checkpoint(
                "/home/cjerzak/Documents/Clay/clay-v1-base.ckpt", 
                metadata_path="/home/cjerzak/Documents/model/configs/metadata.yaml", 
                shuffle=F,  mask_ratio=0) 
            }
            #})
          }
          if(!"RunOnDevice" %in% ls(.GlobalEnv)){
            torch$set_default_device(
              RunOnDevice <<- ifelse(torch$cuda$is_available(), 
                                     yes = list(torch$device("cuda")), 
                                     no = list(torch$device("cpu")))[[1]] 
            )
            torch$set_default_dtype( RunDtype <<- torch$float32 ); 
            ClayModel <<- ClayModel$to(RunOnDevice)
            
            nParameters_Pretrained <<- reticulate::as_iterator(  ClayModel$model$encoder$parameters() )
            nParameters_Pretrained_ <- 0; 
            DoneLoop <- F; while(!DoneLoop){ 
              theNextN <- try(reticulate::iter_next( nParameters_Pretrained )$numel(), T)
              if('integer' %in% class(theNextN)){ nParameters_Pretrained_ <- nParameters_Pretrained_ + theNextN  }
              if(!'integer' %in% class(theNextN)){ DoneLoop <- T }
            }
            nParameters_Pretrained <<- nParameters_Pretrained_
          }
          
          # get place embeddings 
          latlong_embed <- cbind(lat[batch_indices] * pi / 180, long[batch_indices] * pi / 180)
          latlong_embed <-  cbind(sin(latlong_embed[,1]),cos(latlong_embed[,1]),
                                  sin(latlong_embed[,2]),cos(latlong_embed[,2]))
          if(dataType == "video"){
            latlong_embed <- do.call(rbind, unlist(apply(latlong_embed,1,function(zer){
              list(cbind(rep(zer[1], times = np$array(m$shape)[1]/m_shape_orig[[1]]),
                         rep(zer[2], times = np$array(m$shape)[1]/m_shape_orig[[1]]),
                         rep(zer[3], times = np$array(m$shape)[1]/m_shape_orig[[1]]),
                         rep(zer[4], times = np$array(m$shape)[1]/m_shape_orig[[1]])))
            }), recursive = F))
            #latlong_embed <- np$array( jnp$reshape(jnp$concatenate( list(jnp$expand_dims(jnp$array(latlong_embed),1L), jnp$expand_dims(jnp$array(latlong_embed),1L)), 1L), list(-1L,4L) )) 
          }
          time_embed <- t(replicate(np$array(m$shape)[1],{c(-0.1205, -0.9927,  0.2588, -0.9659)})) 
          
          # obtain embeddings 
          # ClayModel$model$metadata$`landsat-c2l1`
          # RGB means: 10678.0, 10563.0, 11083.0
          # RGB stds: 9578.0, 9408.0, 10144.0
          # ClayModel$model$metadata$`landsat-c2l2-sr`

          # resize to 256 if image is greater in size 
          #if(m$shape[[2]] > 256L){
            #m <- jax$image$resize(
              #image=m, shape=c(m$shape[[1]],  256L, 256L, m$shape[[3]]),
              #method="bilinear")
          #}
          
          m <- ClayModel$model$encoder(
            dict("platform" = "landsat-c2l1",  # platform
                 "time" = torch$tensor( time_embed, dtype = RunDtype)$to(RunOnDevice), # temporal embedding 
                 "latlon" = torch$tensor( latlong_embed, dtype = RunDtype )$to(RunOnDevice), # lat long embedding 
                 #"pixels" = torch$tensor( reticulate::np_array(m$transpose(c(0L,3L,1L,2L))), dtype = RunDtype )$to(RunOnDevice), # normalized image 
                 "pixels" = torch$tensor( reticulate::np_array(tf$constant(m$transpose(c(0L,3L,1L,2L)))), dtype = torch$float32),
                 "gsd" = torch$tensor(30, dtype = RunDtype)$to(RunOnDevice),  # resolution 
                 'waves' = torch$tensor(c(0.65, 0.56, 0.48), dtype = RunDtype)$to(RunOnDevice)  # wavelength in micrometers?, this assumes RGB
                 #'waves' = torch$tensor(c(0.493, 0.560, 0.665), dtype = RunDtype)$to(RunOnDevice)  # wavelength in micrometers?, this assumes BGR
            )
          )[[1]]  
          # The first embedding is the [CLS], which is a global embedding
          m = jnp$array(  m$cpu()$detach()$numpy()[,1,] ) 
          # plot(np$array(m)[,sample(1:10,2)])
        }
        if( !grepl(pretrainedModel,pattern="video") & dataType == "video" ){ 
          # reshape if not using videomae
          m <- jnp$reshape(jnp$array(m), list(m_shape_orig[[1]], m_shape_orig[[2]], -1L) ) 
        } 
        if(grepl(pretrainedModel,pattern="videomae")){ 
          if(!"FeatureExtractor" %in% ls(.GlobalEnv) ){  # https://huggingface.co/docs/transformers/en/model_doc/videomae
            print("Loading pre-trained model (videomae)...")
            PretrainedVideoModelName <<- "MCG-NJU/videomae-base"
            #videoModelName <<- "MCG-NJU/videomae-base-finetuned-kinetics"
            
            TransformersModule <<- reticulate::import("transformers") 
            torch <<- reticulate::import("torch") 
            
            # set device and dtypes
            torch$set_default_device(
              RunOnDevice <<- ifelse(torch$cuda$is_available(), 
                                     yes = list(torch$device("cuda")), 
                                     no = list(torch$device("cpu")))[[1]] 
            )
            torch$set_default_dtype( RunDtype <<- torch$float32 ); 
            
            # load models
            py_gc$collect()
            # use VideoMAEImageProcessor? 
            FeatureExtractor <<- TransformersModule$ViTImageProcessor$from_pretrained(PretrainedVideoModelName)
            TransformersModel <<- TransformersModule$ViTModel$from_pretrained(PretrainedVideoModelName, 
                                                                              torch_dtype = torch$float16)#half()$to(RunOnDevice)
            nParameters_Pretrained <<- TransformersModel$num_parameters()
            
            MEAN_RESCALER <<- jnp$array(c(1,1,1))
            MEAN_RESCALER <<- jnp$reshape(MEAN_RESCALER,list(1L,1L,1L,1L,3L))
            
            SD_RESCALER <<- jnp$array(c(1,1,1))
            SD_RESCALER <<- jnp$reshape(SD_RESCALER,list(1L,1L,1L,1L,3L))
            
            NORM_MEAN_array_inner <<- jnp$reshape(jnp$array(NORM_MEAN),list(1L,1L,1L,1L,3L))
            NORM_SD_array_inner <<- jnp$reshape(jnp$array(NORM_SD),list(1L,1L,1L,1L, 3L))
          }
          #m <- reticulate::np_array( tf$constant(m), dtype = np$uint8)
          
          m <- (m - NORM_MEAN_array_inner) / NORM_SD_array_inner
          m <- jax$image$resize(
            image=m,
            shape=c(m$shape[[1]], m$shape[[2]],  224L, 224L, 3L),
            method="bilinear")
          #m <- FeatureExtractor(images = m, return_tensors="pt",do_resize = T, do_rescale = F, do_normalize = F)["pixel_values"]$type(RunDtype)$to(RunOnDevice)
          m <- (m*SD_RESCALER)+MEAN_RESCALER
          # jnp$mean(jnp$array(m), axis = c(0L,2L:3L)); jnp$std(jnp$array(m), axis =  c(0L,2L:3L)) 
          
          #m <- jnp$array(m, dtype = np$uint8)
          m_rep <- c(); for(j_ in 1L:as.integer(m$shape[[1]]) ){ # iterate over the batch dimension? 
            m_ <- jnp$take(m, j_-1L, axis = 0L)
            if(m_$shape[[4]] == 1L){ m_ <- m_ * jnp$expand_dims(jnp$expand_dims(jnp$array(t(c(1,1,1))),0L),0L)$astype(m$dtype) }
            if(m_$shape[[4]] > 3L){ m_ <- jnp$take(m,0L:2L,axis=3L) }
            m_ <- jnp$transpose(  m_, c(0L,3L,1L,2L))
            m_ <- torch$tensor( reticulate::np_array( tf$constant(m_, tf$float32), dtype = np$float32), dtype = torch$float32)
            
            # run model
            # output of extractor is T by C by W by H
            m_ <- TransformersModel(m_)$pooler_output$cpu()$detach()$numpy()
            
            # save final data 
            m_rep <- rbind( m_rep, c(colMeans(m_), apply(m_, 2,sd) ))
          }
          m <- jnp$array( m_rep )
        }
        return( m ) 
      }
    }
    
    # get norm stats if needed 
    if(is.null(NORM_MEAN)){ 
      NORM_MEAN <- GetMoments(ds_iterator_train, dataType = dataType, image_dtype = image_dtype, momentCalIters = 34)
      NORM_SD <- NORM_MEAN$NORM_SD_array; NORM_MEAN <- NORM_MEAN$NORM_MEAN
    }
    
    # define a transformer backbone 
    TransformerBackbone <- function(ModelList, m, StateList, seed, MPList, inference, type){
      if(type == "Spatial"){ # patch embed
          m <- ModelList$SpatialTransformerSupp$PatchEmbedder(jnp$transpose(m, c(2L, 0L, 1L)))
          m <- jnp$transpose(jnp$reshape(m, list(m$shape[[1]],-1L)))
          print2(sprintf("Transformer dims: [%s]", paste(unlist(m$shape),collapse=",")))
      }
      
      if(NeuralizeScale == T){ 
        print2("Neuralizing scale...")
        m_orig <- m
      }

      # append start and stop tokens
      m <- jnp$concatenate(list( eval(parse(text = sprintf("ModelList$%sTransformerSupp$StartEmbed",type))), m), 0L)
      m <- jnp$concatenate(list(m,  eval(parse(text = sprintf("ModelList$%sTransformerSupp$StartEmbed",type)))), 0L) 

      # start neural block
      DepthOfThisTransformer <- ifelse(type=="Spatial", yes = nDepth_ImageRep, no = nDepth_TemporalRep)
      print2(sprintf("Starting Transformer block [depth %s, %s]...", DepthOfThisTransformer, type))
      mtm1 <- m; for(d_ in 1L:DepthOfThisTransformer){

          ModelList_d <- eval(parse(text = sprintf("ModelList$%s_d%s", type, d_) ))
          
          # standardize
          m <- RMS_norm(m) * ModelList_d$TransformerRenormer$NormScaler1

          # rotary embeddings
          if(type == "Spatial"){ m_pos <- RotaryPositionalEmbeddings_spatial( m ) } 
          if(type == "Temporal"){ m_pos <- RotaryPositionalEmbeddings_temporal( m ) } 

          # multihead attention block
          m <- ModelList_d$Multihead(
                      query = m_pos,
                      key_  = m_pos,
                      value = m) #key = seed, # breaks in GPU mode

          # residual connection
          mtm1 <- m <- mtm1 + m*jax$nn$softplus( 
            ModelList_d$ResidualWts$RightWt1$astype(jnp$float32) )$astype(mtm1$dtype)

          # normalize
          m <- RMS_norm(m)*ModelList_d$TransformerRenormer$NormScaler2

          # feed forward
          m <- jax$nn$swish(ffmap(ModelList_d$FF$FFWide1, m)) *
                        ffmap(ModelList_d$FF$FFWide2, m) # swiglu proj
          m <- ffmap(ModelList_d$FF$FFNarrow, m) # final proj

          # residual connection to previous state
          mtm1 <- m <- mtm1 + m*jax$nn$softplus( 
                      ModelList_d$ResidualWts$RightWt2$astype(jnp$float32) )$astype(mtm1$dtype)
      }
      print2(sprintf("Done with Transformer block [depth %s, %s]...", DepthOfThisTransformer, type))

      # take CLS embedding from position 0 [Start]
      m <- jnp$take(m, indices = 0L, axis = 0L)

      # path control -- only executed in final pass over sequences
      if( (dataType == "image" & type == "Spatial") |  (type == "Temporal") ){
        # final norm
        m <- jnp$squeeze( RMS_norm( jnp$expand_dims(m,0L) ) *
                           eval(parse(text = sprintf("ModelList$%sTransformerSupp$FinalNormScaler",type))) )

        # linear proj, note: dense starts with linear projection  
        # m <- eval(parse(text = sprintf("ModelList$%sTransformerSupp$FinalProj",type)))( m )  
        
        if(NeuralizeScale){
          m <- jnp$sum(m_orig * 
             jnp$expand_dims(jax$nn$softmax(  jnp$take(m,jnp$array(0L:(nScalePatches-1L)))  ), 1L), axis = 0L)
        }
      }
    print2("Returning output in TransformerBackbone()")
    return( list(m, StateList) )
    }
    
    # Spatial backbone 
    DoFineTuning <- grepl(pretrainedModel,pattern="-ft")
    if(length(DoFineTuning) == 0){ DoFineTuning <- FALSE}
    if(DoFineTuning){
      setwd(orig_wd); batch_inference_ <- GetElementFromTfRecordAtIndices( uniqueKeyIndices = 1:batchSize,
                                                                          filename = file,
                                                                          nObs = length(unique(imageKeysOfUnits)),
                                                                          return_iterator = T,
                                                                          readVideo = useVideo,
                                                                          image_dtype = image_dtype_tf,
                                                                          iterator = NULL); setwd(new_wd)
      InitImageProcess(jnp$array( batch_inference_[[1]][[1]]), jax$random$PRNGKey(ai(2L+1 + seed)), inference = T);rm(batch_inference_)
      
      ModelList <- c("FTParams"= list(FeatureExtractor$params),
                     "FTParams_NormRescaler"= jnp$array(t(rep(1,times = nWidth_ImageRep))),
                     "FTParams_Proj"= eq$nn$Linear(in_features = ai(nWidth_ImageRep),
                                                   out_features = ai(nWidth_ImageRep),
                                                   use_bias = F, # hidden bias
                                                   key = jax$random$PRNGKey(ai(33440L)))
                     )
      StateList <- list("None"=jnp$array(.0)) # initialize with 0's
      FTBackbone <- function(ModelList, m, StateList, seed, MPList, inference, type){
        #m <- FeatureExtractor(
        m <- FeatureExtractor$get_image_features(
                pixel_values = jnp$expand_dims(m,0L),
                params = ModelList$FTParams,
                dropout_rng = seed,
                train = !inference # with dropout 
        )
        #      )$pooler_output
        m <- LayerNorm(m)*ModelList$FTParams_NormRescaler
        # m <- ffmap(ModelList$FTParams_Proj,m)
        m <- jnp$squeeze(m, 0L)
        return( list(m, jnp$array(1)) ) # representation, state 
      }
    }
    
    if(!DoFineTuning){
    # Transformer backbone 
    if(imageModelClass == "VisionTransformer"){
      StateList <- ModelList <- replicate(nDepth_ImageRep, jnp$array(0.)) # initialize with 0's
      for(d_ in 1L:nDepth_ImageRep){
          TransformerRenormer_d <- list("NormScaler1"=jnp$array( t(rep(1,times=nWidth_ImageRep) ) ),
                                               "NormScaler2"=jnp$array( t(rep(1,times=nWidth_ImageRep) ) ))
          Multihead_d <- eq$nn$MultiheadAttention(
                                    query_size = nWidth_ImageRep,
                                    output_size = nWidth_ImageRep,
                                    num_heads = 12L,
                                    use_output_bias = F,
                                    key = jax$random$PRNGKey( 23453355L + seed + d_) )
          FF_d <- list("FFWide1"=eq$nn$Linear(in_features = nWidth_ImageRep,
                                           out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                           use_bias = F, # hidden bias
                                           key = jax$random$PRNGKey(ai(3340L + seed + d_))),
                              "FFWide2"=eq$nn$Linear(in_features = nWidth_ImageRep,
                                           out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                           use_bias = F, # swiglu bias
                                           key = jax$random$PRNGKey(ai(3311L + seed + d_))),
                              "FFNarrow"=eq$nn$Linear(in_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                           out_features = nWidth_ImageRep,
                                           use_bias = F, # final bias
                                           key = jax$random$PRNGKey(ai(33324L + seed +  d_))))
          StateList[[d_]] <- list('BNState_ImRep'= jnp$array(0.))
          ModelList[[d_]] <- list("Multihead" = Multihead_d,
                                  "FF" = FF_d,
                                  "ResidualWts" = list("RightWt1"=InvSoftPlus(NonLinearScaler),
                                                       "RightWt2"=InvSoftPlus(NonLinearScaler)),
                                  "TransformerRenormer" = TransformerRenormer_d)
      }
      names(ModelList) <- names(StateList) <- paste0("Spatial_d",nDepth_ImageRep)
      ModelList$SpatialTransformerSupp = list(
          "StartEmbed" = jax$random$uniform(key = jax$random$PRNGKey(ai(333324L + seed +  d_)), minval = -sqrt(6/nWidth_ImageRep), maxval = sqrt(6/nWidth_ImageRep), shape = list(1L,nWidth_ImageRep)), # Start
          "StopEmbed" =jax$random$uniform(key = jax$random$PRNGKey(ai(3332124L + seed +  d_)),minval = -sqrt(6/nWidth_ImageRep), maxval = sqrt(6/nWidth_ImageRep), shape = list(1L,nWidth_ImageRep)), # Stop
          "PatchEmbedder" = eq$nn$Conv(kernel_size = c(patchEmbedDim, patchEmbedDim),
                     num_spatial_dims = 2L, stride = c(patchEmbedDim,patchEmbedDim),
                     padding_mode = "REFLECT",
                     in_channels = rawChannelDims, use_bias = F,
                     out_channels = nWidth_ImageRep, key = jax$random$PRNGKey(4L+1040L+seed)), # patch embed
          "FinalNormScaler" = jnp$array(rep(1,times = nWidth_ImageRep)),  # RMS weighter
          "FinalProj" = eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nTransformerOutputWidth,
                        use_bias = F, key = jax$random$PRNGKey(999L + seed  ) ) # final dense proj
        )
    }
    
    # CNN backbone 
    if(imageModelClass == "CNN"){
    StateList <- ModelList <- replicate(nDepth_ImageRep, list())
    for(d_ in 1L:nDepth_ImageRep){
      if(d_ > 1){ strides <- 1L }
      SeperableSpatial <- eq$nn$Conv(kernel_size = c(kernelSize, kernelSize),
                                         num_spatial_dims = 2L, stride = c(strides, strides),
                                         in_channels = dimsSpatial <- ai(ifelse(d_ == 1L, yes = rawChannelDims, no = nWidth_ImageRep)),
                                         out_channels = dimsSpatial, use_bias = F,
                                         groups = dimsSpatial,
                                         key = jax$random$PRNGKey(4L+d_+seed))
      SeperableFeature <- eq$nn$Conv(in_channels = dimsSpatial, 
                                         out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                         groups = 1L, 
                                         num_spatial_dims = 2L,stride = c(1L,1L), use_bias = T,
                                         key = jax$random$PRNGKey(50L+d_+seed))
      SeperableFeature2 <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                         out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                         groups = 1L, 
                                         num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                         key = jax$random$PRNGKey(530L+d_+seed))
      SeperableFeature3 <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                          out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                          groups = 1L, 
                                          num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                          key = jax$random$PRNGKey(5340L+d_+seed))
      SeperableFeature4 <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                          out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                          groups = 1L, 
                                          num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                          key = jax$random$PRNGKey(53140L+d_+seed))
      ResidualTm1Path <- eq$nn$Conv(in_channels = dimsSpatial, 
                                        out_channels = nWidth_ImageRep,
                                        groups = 1L,
                                        kernel_size = c(1L,1L),
                                        num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                        key = jax$random$PRNGKey(3250L+d_+seed))
      ResidualTPath <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                      out_channels = nWidth_ImageRep,
                                      groups = 1L,
                                      kernel_size = c(1L,1L),
                                      num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                      key = jax$random$PRNGKey(32520L+d_+seed))

        # setup bn for CNN block
        LayerBN1 <- eq$nn$BatchNorm(
          input_size = (BatchNormDim1 <- nWidth_ImageRep), # post-process norm
          axis_name = batch_axis_name,
          momentum = bn_momentum, eps = (BN_ep <- 0.01^2), channelwise_affine = F)
        LayerBN2 <- eq$nn$BatchNorm(
          input_size = (BatchNormDim2 <- nWidth_ImageRep), 
          axis_name = batch_axis_name,
          momentum = bn_momentum, eps = BN_ep, channelwise_affine = F)
        StateList[[d_]] <- list('BNState1_ImRep'=eq$nn$State( LayerBN1 ),
                                'BNState2_ImRep'=eq$nn$State( LayerBN2 ))
        ModelList[[d_]] <- list("SeperableSpatial" = SeperableSpatial,
                                  "SeperableFeature" = SeperableFeature,
                                  "SeperableFeature2" = SeperableFeature2, 
                                  "SeperableFeature3" = SeperableFeature3, 
                                  "SeperableFeature4" = SeperableFeature4, 
                                  "ResidualTm1Path" = ResidualTm1Path,
                                  "ResidualTPath" = ResidualTPath,
                                  "SpatialResidualWts" = list("RightWt1" = InvSoftPlus( NonLinearScaler ),
                                                              "RightWt2" = InvSoftPlus( NonLinearScaler )),
                                  "BN1_ImRep" = list("BN" = LayerBN1, "BNRescaler" = jnp$array(rep(1.,times=BatchNormDim1)) ),
                                  "BN2_ImRep" = list("BN" = LayerBN2, "BNRescaler" = jnp$array(rep(1.,times=BatchNormDim2)) ))
    }
    names(ModelList) <- names(StateList) <- paste0("Spatial_d",nDepth_ImageRep)
    
    ModelList$SpatialTransformerSupp = list(
      "FinalCNNProj"=eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nWidth_ImageRep, # final dense proj
                          use_bias = F, key = jax$random$PRNGKey(9989L + seed  ) )  ,
      "FinalCNNBN"= list("BN" = (LayerBN <- eq$nn$BatchNorm(input_size = nWidth_ImageRep,
                            axis_name = batch_axis_name,
                            momentum = bn_momentum, eps = BN_ep, channelwise_affine = F)), 
                         "BNRescaler" = jnp$array(rep(1.,times=nWidth_ImageRep) ) ) 
    )
    StateList$BNState_ImRep_FinalCNNBN <- eq$nn$State( LayerBN )
    }
    }
    
    # Temporal backbone
    if(dataType == "video" | NeuralizeScale){
      for(dt_ in 1L:nDepth_TemporalRep){
        TransformerRenormer_d <- list("NormScaler1" = jnp$array( t(rep(1,times=nWidth_VideoRep) ) ),
                                      "NormScaler2" = jnp$array( t(rep(1,times=nWidth_VideoRep) ) ))
        Multihead_d <- eq$nn$MultiheadAttention(
                                    query_size = nWidth_VideoRep,
                                    output_size = nWidth_VideoRep,
                                    num_heads = 8L,
                                    use_output_bias = F,
                                    key = jax$random$PRNGKey( 2343355L + seed+dt_) )
        FF_d  <- list(
          "FFWide1" = eq$nn$Linear(in_features = nWidth_VideoRep,
                          out_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
                          use_bias = F, # hidden bias
                          key = jax$random$PRNGKey(ai(334300L + 1L+dt_ + seed  ))),
          "FFWide2" = eq$nn$Linear(in_features = nWidth_VideoRep,
                          out_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
                          use_bias = F, # swiglu bias
                          key = jax$random$PRNGKey(ai(333110L + 1L+dt_ + seed ))),
          "FFNarrow" = eq$nn$Linear(in_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
                          out_features = nWidth_VideoRep,
                          use_bias = F, # final bias
                          key = jax$random$PRNGKey(ai(3333924L + 1L + dt_ + seed ))))
        eval(parse(text = sprintf('ModelList$Temporal_d%s <- 
                        list("TransformerRenormer" = TransformerRenormer_d,
                             "Multihead" = Multihead_d,
                             "ResidualWts" = list("RightWt1" = InvSoftPlus( NonLinearScaler ), 
                                                  "RightWt2" = InvSoftPlus( NonLinearScaler )),
                             "FF" = FF_d)', dt_)))
      }
      
      ModelList$TemporalTransformerSupp = list(
        "StartEmbed" = jax$random$uniform(key = jax$random$PRNGKey(ai(33932124L + seed +  dt_)),
                                     minval = -sqrt(6/nWidth_VideoRep), maxval = sqrt(6/nWidth_VideoRep), shape = list(1L,nWidth_VideoRep)), # start 
        "StopEmbed" =  jax$random$uniform(key = jax$random$PRNGKey(ai(3324L + seed +  dt_)), 
                                     minval = -sqrt(6/nWidth_VideoRep), maxval = sqrt(6/nWidth_VideoRep), shape = list(1L,nWidth_VideoRep)), # stop
        "PatchEmbedder" =  jnp$array(0.), # unused  in temporal
        "FinalNormScaler" =  jnp$array( t(rep(1,times=nWidth_VideoRep) ) ),
        "FinalProj" =  eq$nn$Linear(in_features = nWidth_VideoRep, out_features =  nWidth_VideoRep,
                               use_bias = F, key = jax$random$PRNGKey(1999L+dt_+seed  ))
        )
    }
    
    # Combine all entities
    # m <- InitImageProcess( jnp$array( batch_inference[[1]]),T)[0,0,,,];  d__ <- 1L; inference <- F
    # m <- InitImageProcess( jnp$array( batch_inference[[1]]), T);  d__ <- 1L; inference <- F
    ImageRepArm_SpatialArm <- function(ModelList, m, StateList, seed, MPList, inference){
      if(DoFineTuning){
          if( NeuralizeScale ){ 
            # Define the patchify_array function in R
            patchify_array <- function(arr, h_patches = 3L, w_patches = 3L){
              # Get the shape of the input array
              arr_shape <- arr$shape
              X <- arr_shape[[1]]
              H <- arr_shape[[2]]
              W <- arr_shape[[3]]
              
              # Calculate the padding needed for height and width
              pad_h <- (-H) %% h_patches
              pad_w <- (-W) %% w_patches
              
              # Pad the array using jax.numpy.pad
              arr_padded <- jnp$pad(
                arr,
                pad_width = list(
                  c(0L, 0L),      # No padding on the batch dimension
                  c(0L, pad_h),   # Pad height dimension
                  c(0L, pad_w)    # Pad width dimension
                ),
                mode = "constant"
              )
              
              # Update the padded dimensions
              H_padded <- H + pad_h
              W_padded <- W + pad_w
              
              # Compute the size of each patch
              patch_size_h <- as.integer(H_padded / h_patches)
              patch_size_w <- as.integer(W_padded / w_patches)
              
              # Reshape and transpose the array
              arr_reshaped <- arr_padded$reshape(
                as.integer(c(X, h_patches, patch_size_h, w_patches, patch_size_w))
              )
              arr_patched <- jnp$transpose(
                arr_reshaped,
                as.integer(c(0L, 1L, 3L, 2L, 4L))
              )
              
              # Return the patched array
              return(arr_patched)
            }
            nChannels <- m$shape[[1]]
            m <- patchify_array(m) # nChannels by patchx by patchy by spatial 
            m <- jax$vmap(jax$vmap(function(m_){
              jax$image$resize( image=m_,
                                shape=c(3L, 224L, 224L), method="bilinear")
            }, in_axes = list(1L)),in_axes = list(2L))( m )
            m <- jax$vmap( jax$vmap(FTBackbone,
                                      list(NULL, 1L, NULL, NULL, NULL, NULL, NULL) ), 
                             list(NULL, 2L, NULL, NULL, NULL, NULL, NULL) )(
                          ModelList, m, StateList, seed, MPList, inference, "Spatial" )
            m[[1]] <- jnp$reshape(m[[1]], shape = list(-1L, nWidth_ImageRep))
          }
          if( !NeuralizeScale ){ 
            m <- FTBackbone(ModelList, m, StateList, seed, MPList, inference, type = "Spatial")
          }
          StateList <- jnp$array(1.); m <- m[[1]]
      }
      if(!DoFineTuning){
        if(imageModelClass == "VisionTransformer" ){
          m <- TransformerBackbone(ModelList, m, StateList, seed, MPList, inference, type = "Spatial")
          StateList <- m[[2]]; m <- m[[1]]
        }
        if(imageModelClass == "CNN" ){
            m <- jnp$transpose(m, c(2L, 0L, 1L)) # transpose to CWH
            for(d__ in 1:nDepth_ImageRep){
              print(sprintf("CNN depth %s of %s",d__,nDepth_ImageRep))
              
              eval(parse(text = sprintf("ModelList_d <- ModelList$Spatial_d%s", d__)))
              eval(parse(text = sprintf("StateList_d <- StateList$Spatial_d%s", d__)))
              
              # residual path 
              mtm1 <- m
              
              # seperable spatial convolution   
              m <- ModelList_d$SeperableFeature(
                         ModelList_d$SeperableSpatial( m ) )
  
              if(! optimizeImageRep){
                # hist(np$array(m$val)) 
                
                m <- jnp$concatenate(list(mx_ <- jnp$max(m, c(1L:2L)), 
                                          mn_ <- jnp$mean(m, c(1L:2L)), 
                                          mx_ * mn_), 0L)
                return( list(m, StateList)  )
              }
              
              if( optimizeImageRep ){
                if( T == T ){ 
                  m <- ModelList_d$BN1_ImRep$BN(m, state = StateList_d$BNState1_ImRep, 
                                                inference = inference)
                  eval(parse(text = sprintf("StateList$Spatial_d%s$BNState1_ImRep <- m[[2]]", d__)))
                  
                  # rescale 
                  m <- m[[1]] * jnp$expand_dims(jnp$expand_dims( ModelList_d$BN1_ImRep$BNRescaler,1L),1L)
                }
                
                # swiglu 
                m <- jax$nn$swish(  ModelList_d$SeperableFeature2(m) )  * ModelList_d$SeperableFeature3(m)
                m <- ModelList_d$SeperableFeature4(m)
                
                # adaptive max pooling 
                if(d__ %% 2 %in% c(0,1)){ 
                  m <- eq$nn$AdaptiveMaxPool2d(list(ai(m$shape[[2]]*(SpatialShrinkRate <- 0.75)), 
                                                    ai(m$shape[[3]]*SpatialShrinkRate)))( m ) 
                  m <- ModelList_d$ResidualTPath(m)
                }
                
                if(T == F){ 
                  hist(c(np$array(mtm1$val))); apply(np$array(mtm1$val),2,sd)
                  hist(c(np$array(m$val))); apply(np$array(m$val),2,sd)
                }
                
                # residual connection 
                m <- eq$nn$AdaptiveAvgPool2d(list(m$shape[[2]],m$shape[[3]]))(
                        ModelList_d$ResidualTm1Path(mtm1)) +  
                           m * jax$nn$softplus( ModelList_d$SpatialResidualWts$RightWt1$astype(jnp$float32)
                                                )$astype(mtm1$dtype)
              }
            }
            
            # final pooling, final norm (if used), and projection 
            m <- jnp$max(m, c(1L:2L))
            if(optimizeImageRep & T == T){
                print2("Performing final CNN normalization...")
                m <- ModelList$SpatialTransformerSupp$FinalCNNBN$BN(m, 
                                                                    state = StateList$BNState_ImRep_FinalCNNBN, 
                                                                    inference = inference)
                StateList$BNState_ImRep_FinalCNNBN <- m[[2]]
                m <- m[[1]] * ModelList$SpatialTransformerSupp$FinalCNNBN$BNRescaler
            }
        }
      }
      print2("Returning ImageRepArm_SpatialArm outputs...")
      return( list(m, StateList)  )
    }
    
    #ImageRepArm_batch <-                (ImageRepArm_batch_R <- function(ModelList, m, StateList, seed, MPList, inference){ print("DEBUG MODE IS ON IN IMAGE BACKBONE"); Sys.sleep(5L); 
    ImageRepArm_batch <- eq$filter_jit(ImageRepArm_batch_R <- function(ModelList, m, StateList, seed, MPList, inference){
      ModelList <- MPList[[1]]$cast_to_compute( ModelList )
      StateList <- MPList[[1]]$cast_to_compute( StateList )

      # squeeze temporal dim if needed
      thisPath <- T; if(!is.null(pretrainedModel)){ thisPath <- !grepl(pretrainedModel,pattern="video") }
      if(thisPath){
        if(dataType == "video" & is.null(pretrainedModel)){ m <- jnp$reshape(m, c(-1L, (orig_shape_m <- jnp$shape(m))[3:5])) }
  
        print2(sprintf("Image stack dims: [%s]", paste(unlist(m$shape),collapse=",")))
  
        # get spatial representation if custom architecture 
        if( is.null(pretrainedModel) ){ 
          m <- jax$vmap(function(ModelList, m,
                                 StateList, seed, MPList, inference){
                ImageRepArm_SpatialArm(ModelList, m,
                                       StateList, seed, MPList, inference) },
                   in_axes = list(NULL, 0L, NULL, NULL, NULL, NULL),
                   axis_name = batch_axis_name,
                   out_axes = list(0L,NULL))(ModelList, m, StateList, seed, MPList, inference)
          StateList <- m[[2]]; m <- m[[1]]
        }
  
        # unsqueeze temporal dim if needed
        if(dataType == "video" | NeuralizeScale){
          if( is.null(pretrainedModel) ){ m <- jnp$reshape(m, c(orig_shape_m[1:2], -1L)) } 
          # (np$array(m)[1:5,,sample(1:10,1)]); m$shape
          m <- jax$vmap(function(ModelList, m,
                                 StateList, seed, MPList, inference){
                      TransformerBackbone(ModelList, m,
                                          StateList, seed, MPList, inference, type = "Temporal")},
                        in_axes = list(NULL, 0L, NULL, NULL, NULL, NULL),
                        axis_name = batch_axis_name,
                        out_axes = list(0L,NULL))(ModelList, m,
                                                  StateList, jnp$add(seed,42L), MPList, inference)
          StateList <- m[[2]]; m <- m[[1]]
          print2("Returning ImageRepArm_TemporalArm outputs...")
        }
      }
      print2("Returning ImageRepArm_batch outputs...")
      return( list(m, StateList) )
    })
  }
  
  Representations <- NULL; if(getRepresentations){
  Representations <- matrix(NA,nrow = length(unique(imageKeysOfUnits)), 
                            ncol = ifelse(optimizeImageRep, yes = nWidth_ImageRep, 
                                                            no = nWidth_ImageRep+2L*nWidth_ImageRep*(imageModelClass=="CNN")*is.null(pretrainedModel) ))
  usedImageKeys <- c(); last_i <- 0; ok_counter <- 0; ok<-F; while(!ok){
      ok_counter <- ok_counter + 1

      batch_indices <- (last_i+1):(last_i+batchSize)
      batch_indices <- batch_indices[batch_indices <= length(unique(imageKeysOfUnits))]
      last_i <- batch_indices[ length(batch_indices) ]

      # checks for last / batch size corrections
      if(last_i == length(unique(imageKeysOfUnits))){ ok <- T }
      batchSizeOneCorrection <- ifelse(length(batch_indices) == 1, yes = TRUE, no = FALSE)

      # get the data
      setwd(orig_wd); batch_inference <- GetElementFromTfRecordAtIndices( uniqueKeyIndices = batch_indices,
                                                            filename = file,
                                                            nObs = length(unique(imageKeysOfUnits)),
                                                            return_iterator = T,
                                                            readVideo = useVideo,
                                                            image_dtype = image_dtype_tf,
                                                            iterator = ifelse(ok_counter > 1,
                                                                              yes = list(saved_iterator),
                                                                              no = list(NULL))[[1]] ); setwd(new_wd)
      if(batchSizeOneCorrection){
          batch_indices <- c(batch_indices,batch_indices)
          batch_inference[[1]][[1]] <- tf$concat(list(tf$expand_dims(batch_inference[[1]][[1]],0L),
                                                      tf$expand_dims(batch_inference[[1]][[1]],0L)), 0L)
      }
      saved_iterator <- batch_inference[[2]]
      batch_inference <- batch_inference[[1]]
      batch_keys <- unlist(  lapply( p2l(batch_inference[[3]]$numpy()), as.character) )

      gc(); py_gc$collect() # collect memory
      # im <- jnp$array(batch_inference[[1]]); seed <- jax$random$PRNGKey(ai(2L+ok_counter + seed)); inference = T
      # plot( np$array( jnp$array(batch_inference[[1]]))[,,1,3])# check variability across units
      # batch_inference[[1]][3,,,1] # check image inputs 
      representation_ <-  np$array( ImageRepArm_batch(ModelList,
                                                      InitImageProcess(jnp$array(batch_inference[[1]]),
                                                                       jax$random$PRNGKey(ai(2L+ok_counter + seed)), inference = T),
                                                      StateList,
                                                      jax$random$PRNGKey(ai(last_i + seed)),
                                                      MPList, 
                                                      T # inference 
                                                      )[[1]]  )
      
      # plot(representation_[,sample(1:20)]); hist(as.matrix(representation_)); apply(as.matrix(representation_),2,sd)
      if(T == F){ 
        # sanity checks
        tmp <- np$array(jnp$take(jnp$array(batch_inference[[1]]),(iCheck <- 8L)-1L,axis=0L))
        image2(tmp[,,1])
        cbind(lat[which(imageKeysOfUnits %in% batch_keys[ iCheck ])], long[which(imageKeysOfUnits %in% batch_keys[ iCheck ])])
      }
      if(any(is.na(representation_)) &  !initializingFxns){ 
        stop("Stopping due to missingness in intermediary representation_ [Code reference: ImageModelBackbone.R]") 
      }
      if("try-error" %in% class(representation_)){
        print(representation_)
        stop("Stopping due to try-error in *intermediary* version of representation_ [Code reference: ImageModelBackbone.R]") 
      }
      
      if(batchSizeOneCorrection){ representation_ <- representation_[1,] }
      usedImageKeys <- c(usedImageKeys, batch_keys)
      Representations[batch_indices,] <- representation_
      print2(sprintf("%.2f%% done getting image/video representations", 100*last_i / length(unique(imageKeysOfUnits))))
  }
  Representations <- Representations[match(as.character(imageKeysOfUnits), as.character(usedImageKeys) ),]
  print2(sprintf("Done getting image/video representations!"))
  }

  # reset wd (may have been changed via tfrecords use)
  setwd(  orig_wd  )
  
  print2("Obtaining approximate parameter count...")
  nParamsRep <- sum(unlist(lapply(jax$tree_leaves(eq$partition(ModelList, eq$is_array)[[1]]), function(zer){zer$size})))
  if(is.null(pretrainedModel) & optimizeImageRep == F){ nParamsRep <- nParamsRep }
  if(!is.null(pretrainedModel) ){
    if(dataType == "image"){ nParamsRep <- nParameters_Pretrained }
    if(dataType == "video" & !grepl(pretrainedModel,pattern="video")){ nParamsRep <- nParamsRep + nParameters_Pretrained }
    if(dataType == "video" & grepl(pretrainedModel,pattern="video")){ nParamsRep <- nParameters_Pretrained }
  }

  # sanity check 
  if(any(is.na(Representations)) & !initializingFxns){
    stop("Stopping due to missingness in *output* version of Representations [Code reference: ImageModelBackbone.R]")
  }
  
  if(CleanupEnv){
    suppressWarnings( rm(PretrainedImageModelName, FeatureExtractor, ClayModel, TransformersModel, nParameters_Pretrained, pos = .GlobalEnv) ) 
  }
  if(returnContents){
   return( list( "ImageRepresentations"= Representations,
                 "ImageRepArm_batch_R" = ImageRepArm_batch_R,
                 "ImageRepArm_batch" = ImageRepArm_batch,
                 "ImageModel_And_State_And_MPPolicy_List" = list(ModelList, StateList, MPList), 
                 "InitImageProcess" = InitImageProcess,
                 "nParamsRep" = nParamsRep
                 ) )
  }
  suppressWarnings(rm(ModelList, StateList, MPList)); gc()
}


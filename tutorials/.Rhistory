num_heads = 12L,
use_output_bias = F,
key = jax$random$PRNGKey( 23453355L + seed + d_) )
FF_d <- list("FFWide1"=eq$nn$Linear(in_features = nWidth_ImageRep,
out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
use_bias = F, # hidden bias
key = jax$random$PRNGKey(ai(3340L + seed + d_))),
"FFWide2"=eq$nn$Linear(in_features = nWidth_ImageRep,
out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
use_bias = F, # swiglu bias
key = jax$random$PRNGKey(ai(3311L + seed + d_))),
"FFNarrow"=eq$nn$Linear(in_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
out_features = nWidth_ImageRep,
use_bias = F, # final bias
key = jax$random$PRNGKey(ai(33324L + seed +  d_))))
StateList[[d_]] <- list('BNState_ImRep'= jnp$array(0.))
ModelList[[d_]] <- list("Multihead" = Multihead_d,
"FF" = FF_d,
"ResidualWts" = list("RightWt1"=InvSoftPlus(NonLinearScaler),
"RightWt2"=InvSoftPlus(NonLinearScaler)),
"TransformerRenormer" = TransformerRenormer_d)
}
names(ModelList) <- names(StateList) <- paste0("Spatial_d",nDepth_ImageRep)
ModelList$SpatialTransformerSupp = list(
"StartEmbed" = jax$random$uniform(key = jax$random$PRNGKey(ai(333324L + seed +  d_)), minval = -sqrt(6/nWidth_ImageRep), maxval = sqrt(6/nWidth_ImageRep), shape = list(1L,nWidth_ImageRep)), # Start
"StopEmbed" =jax$random$uniform(key = jax$random$PRNGKey(ai(3332124L + seed +  d_)),minval = -sqrt(6/nWidth_ImageRep), maxval = sqrt(6/nWidth_ImageRep), shape = list(1L,nWidth_ImageRep)), # Stop
"PatchEmbedder" = eq$nn$Conv(kernel_size = c(patchEmbedDim, patchEmbedDim),
num_spatial_dims = 2L, stride = c(patchEmbedDim,patchEmbedDim),
padding_mode = "REFLECT",
in_channels = rawChannelDims, use_bias = F,
out_channels = nWidth_ImageRep, key = jax$random$PRNGKey(4L+1040L+seed)), # patch embed
"FinalNormScaler" = jnp$array(rep(1,times = nWidth_ImageRep)),  # RMS weighter
"FinalProj" = eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nTransformerOutputWidth,
use_bias = F, key = jax$random$PRNGKey(999L + seed  ) ) # final dense proj
)
}
# CNN backbone
if(imageModelClass == "CNN"){
StateList <- ModelList <- replicate(nDepth_ImageRep, list())
for(d_ in 1L:nDepth_ImageRep){
if(d_ > 1){ strides <- 1L }
SeperableSpatial <- eq$nn$Conv(kernel_size = c(kernelSize, kernelSize),
num_spatial_dims = 2L, stride = c(strides, strides),
in_channels = dimsSpatial <- ai(ifelse(d_ == 1L, yes = rawChannelDims, no = nWidth_ImageRep)),
out_channels = dimsSpatial, use_bias = F,
groups = dimsSpatial,
key = jax$random$PRNGKey(4L+d_+seed))
SeperableFeature <- eq$nn$Conv(in_channels = dimsSpatial,
out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
groups = 1L,
num_spatial_dims = 2L,stride = c(1L,1L), use_bias = T,
key = jax$random$PRNGKey(50L+d_+seed))
SeperableFeature2 <- eq$nn$Conv(in_channels = nWidth_ImageRep,
out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
groups = 1L,
num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
key = jax$random$PRNGKey(530L+d_+seed))
SeperableFeature3 <- eq$nn$Conv(in_channels = nWidth_ImageRep,
out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
groups = 1L,
num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
key = jax$random$PRNGKey(5340L+d_+seed))
SeperableFeature4 <- eq$nn$Conv(in_channels = nWidth_ImageRep,
out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
groups = 1L,
num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
key = jax$random$PRNGKey(53140L+d_+seed))
ResidualTm1Path <- eq$nn$Conv(in_channels = dimsSpatial,
out_channels = nWidth_ImageRep,
groups = 1L,
kernel_size = c(1L,1L),
num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
key = jax$random$PRNGKey(3250L+d_+seed))
ResidualTPath <- eq$nn$Conv(in_channels = nWidth_ImageRep,
out_channels = nWidth_ImageRep,
groups = 1L,
kernel_size = c(1L,1L),
num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
key = jax$random$PRNGKey(32520L+d_+seed))
# setup bn for CNN block
LayerBN1 <- eq$nn$BatchNorm(
input_size = (BatchNormDim1 <- nWidth_ImageRep), # post-process norm
axis_name = batch_axis_name,
momentum = bn_momentum, eps = (BN_ep <- 0.01^2), channelwise_affine = F)
LayerBN2 <- eq$nn$BatchNorm(
input_size = (BatchNormDim2 <- nWidth_ImageRep),
axis_name = batch_axis_name,
momentum = bn_momentum, eps = BN_ep, channelwise_affine = F)
StateList[[d_]] <- list('BNState1_ImRep'=eq$nn$State( LayerBN1 ),
'BNState2_ImRep'=eq$nn$State( LayerBN2 ))
ModelList[[d_]] <- list("SeperableSpatial" = SeperableSpatial,
"SeperableFeature" = SeperableFeature,
"SeperableFeature2" = SeperableFeature2,
"SeperableFeature3" = SeperableFeature3,
"SeperableFeature4" = SeperableFeature4,
"ResidualTm1Path" = ResidualTm1Path,
"ResidualTPath" = ResidualTPath,
"SpatialResidualWts" = list("RightWt1" = InvSoftPlus( NonLinearScaler ),
"RightWt2" = InvSoftPlus( NonLinearScaler )),
"BN1_ImRep" = list("BN" = LayerBN1, "BNRescaler" = jnp$array(rep(1.,times=BatchNormDim1)) ),
"BN2_ImRep" = list("BN" = LayerBN2, "BNRescaler" = jnp$array(rep(1.,times=BatchNormDim2)) ))
}
names(ModelList) <- names(StateList) <- paste0("Spatial_d",nDepth_ImageRep)
ModelList$SpatialTransformerSupp = list(
"FinalCNNProj"=eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nWidth_ImageRep, # final dense proj
use_bias = F, key = jax$random$PRNGKey(9989L + seed  ) )  ,
"FinalCNNBN"= list("BN" = (LayerBN <- eq$nn$BatchNorm(input_size = nWidth_ImageRep,
axis_name = batch_axis_name,
momentum = bn_momentum, eps = BN_ep, channelwise_affine = F)),
"BNRescaler" = jnp$array(rep(1.,times=nWidth_ImageRep) ) )
)
StateList$BNState_ImRep_FinalCNNBN <- eq$nn$State( LayerBN )
}
}
# Temporal backbone
if(dataType == "video" | NeuralizeScale){
for(dt_ in 1L:nDepth_TemporalRep){
TransformerRenormer_d <- list("NormScaler1" = jnp$array( t(rep(1,times=nWidth_VideoRep) ) ),
"NormScaler2" = jnp$array( t(rep(1,times=nWidth_VideoRep) ) ))
Multihead_d <- eq$nn$MultiheadAttention(
query_size = nWidth_VideoRep,
output_size = nWidth_VideoRep,
num_heads = 8L,
use_output_bias = F,
key = jax$random$PRNGKey( 2343355L + seed+dt_) )
FF_d  <- list(
"FFWide1" = eq$nn$Linear(in_features = nWidth_VideoRep,
out_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
use_bias = F, # hidden bias
key = jax$random$PRNGKey(ai(334300L + 1L+dt_ + seed  ))),
"FFWide2" = eq$nn$Linear(in_features = nWidth_VideoRep,
out_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
use_bias = F, # swiglu bias
key = jax$random$PRNGKey(ai(333110L + 1L+dt_ + seed ))),
"FFNarrow" = eq$nn$Linear(in_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
out_features = nWidth_VideoRep,
use_bias = F, # final bias
key = jax$random$PRNGKey(ai(3333924L + 1L + dt_ + seed ))))
eval(parse(text = sprintf('ModelList$Temporal_d%s <-
list("TransformerRenormer" = TransformerRenormer_d,
"Multihead" = Multihead_d,
"ResidualWts" = list("RightWt1" = InvSoftPlus( NonLinearScaler ),
"RightWt2" = InvSoftPlus( NonLinearScaler )),
"FF" = FF_d)', dt_)))
}
ModelList$TemporalTransformerSupp = list(
"StartEmbed" = jax$random$uniform(key = jax$random$PRNGKey(ai(33932124L + seed +  dt_)),
minval = -sqrt(6/nWidth_VideoRep), maxval = sqrt(6/nWidth_VideoRep), shape = list(1L,nWidth_VideoRep)), # start
"StopEmbed" =  jax$random$uniform(key = jax$random$PRNGKey(ai(3324L + seed +  dt_)),
minval = -sqrt(6/nWidth_VideoRep), maxval = sqrt(6/nWidth_VideoRep), shape = list(1L,nWidth_VideoRep)), # stop
"PatchEmbedder" =  jnp$array(0.), # unused  in temporal
"FinalNormScaler" =  jnp$array( t(rep(1,times=nWidth_VideoRep) ) ),
"FinalProj" =  eq$nn$Linear(in_features = nWidth_VideoRep, out_features =  nWidth_VideoRep,
use_bias = F, key = jax$random$PRNGKey(1999L+dt_+seed  ))
)
}
# Combine all entities
# m <- InitImageProcess( jnp$array( batch_inference[[1]]),T)[0,0,,,];  d__ <- 1L; inference <- F
# m <- InitImageProcess( jnp$array( batch_inference[[1]]), T);  d__ <- 1L; inference <- F
ImageRepArm_SpatialArm <- function(ModelList, m, StateList, seed, MPList, inference){
if(DoFineTuning){
if( NeuralizeScale ){
# Define the patchify_array function in R
patchify_array <- function(arr, h_patches = 3L, w_patches = 3L){
# Get the shape of the input array
arr_shape <- arr$shape
X <- arr_shape[[1]]
H <- arr_shape[[2]]
W <- arr_shape[[3]]
# Calculate the padding needed for height and width
pad_h <- (-H) %% h_patches
pad_w <- (-W) %% w_patches
# Pad the array using jax.numpy.pad
arr_padded <- jnp$pad(
arr,
pad_width = list(
c(0L, 0L),      # No padding on the batch dimension
c(0L, pad_h),   # Pad height dimension
c(0L, pad_w)    # Pad width dimension
),
mode = "constant"
)
# Update the padded dimensions
H_padded <- H + pad_h
W_padded <- W + pad_w
# Compute the size of each patch
patch_size_h <- as.integer(H_padded / h_patches)
patch_size_w <- as.integer(W_padded / w_patches)
# Reshape and transpose the array
arr_reshaped <- arr_padded$reshape(
as.integer(c(X, h_patches, patch_size_h, w_patches, patch_size_w))
)
arr_patched <- jnp$transpose(
arr_reshaped,
as.integer(c(0L, 1L, 3L, 2L, 4L))
)
# Return the patched array
return(arr_patched)
}
nChannels <- m$shape[[1]]
m <- patchify_array(m) # nChannels by patchx by patchy by spatial
m <- jax$vmap(jax$vmap(function(m_){
jax$image$resize( image=m_,
shape=c(3L, 224L, 224L), method="bilinear")
}, in_axes = list(1L)),in_axes = list(2L))( m )
m <- jax$vmap( jax$vmap(FTBackbone,
list(NULL, 1L, NULL, NULL, NULL, NULL, NULL) ),
list(NULL, 2L, NULL, NULL, NULL, NULL, NULL) )(
ModelList, m, StateList, seed, MPList, inference, "Spatial" )
m[[1]] <- jnp$reshape(m[[1]], shape = list(-1L, nWidth_ImageRep))
}
if( !NeuralizeScale ){
m <- FTBackbone(ModelList, m, StateList, seed, MPList, inference, type = "Spatial")
}
StateList <- jnp$array(1.); m <- m[[1]]
}
if(!DoFineTuning){
if(imageModelClass == "VisionTransformer" ){
m <- TransformerBackbone(ModelList, m, StateList, seed, MPList, inference, type = "Spatial")
StateList <- m[[2]]; m <- m[[1]]
}
if(imageModelClass == "CNN" ){
m <- jnp$transpose(m, c(2L, 0L, 1L)) # transpose to CWH
for(d__ in 1:nDepth_ImageRep){
print(sprintf("CNN depth %s of %s",d__,nDepth_ImageRep))
eval(parse(text = sprintf("ModelList_d <- ModelList$Spatial_d%s", d__)))
eval(parse(text = sprintf("StateList_d <- StateList$Spatial_d%s", d__)))
# residual path
mtm1 <- m
# seperable spatial convolution
m <- ModelList_d$SeperableFeature(
ModelList_d$SeperableSpatial( m ) )
if(! optimizeImageRep){
# hist(np$array(m$val))
m <- jnp$concatenate(list(mx_ <- jnp$max(m, c(1L:2L)),
mn_ <- jnp$mean(m, c(1L:2L)),
mx_ * mn_), 0L)
return( list(m, StateList)  )
}
if( optimizeImageRep ){
if( T == T ){
m <- ModelList_d$BN1_ImRep$BN(m, state = StateList_d$BNState1_ImRep,
inference = inference)
eval(parse(text = sprintf("StateList$Spatial_d%s$BNState1_ImRep <- m[[2]]", d__)))
# rescale
m <- m[[1]] * jnp$expand_dims(jnp$expand_dims( ModelList_d$BN1_ImRep$BNRescaler,1L),1L)
}
# swiglu
m <- jax$nn$swish(  ModelList_d$SeperableFeature2(m) )  * ModelList_d$SeperableFeature3(m)
m <- ModelList_d$SeperableFeature4(m)
# adaptive max pooling
if(d__ %% 2 %in% c(0,1)){
m <- eq$nn$AdaptiveMaxPool2d(list(ai(m$shape[[2]]*(SpatialShrinkRate <- 0.75)),
ai(m$shape[[3]]*SpatialShrinkRate)))( m )
m <- ModelList_d$ResidualTPath(m)
}
if(T == F){
hist(c(np$array(mtm1$val))); apply(np$array(mtm1$val),2,sd)
hist(c(np$array(m$val))); apply(np$array(m$val),2,sd)
}
# residual connection
m <- eq$nn$AdaptiveAvgPool2d(list(m$shape[[2]],m$shape[[3]]))(
ModelList_d$ResidualTm1Path(mtm1)) +
m * jax$nn$softplus( ModelList_d$SpatialResidualWts$RightWt1$astype(jnp$float32)
)$astype(mtm1$dtype)
}
}
# final pooling, final norm (if used), and projection
m <- jnp$max(m, c(1L:2L))
if(optimizeImageRep & T == T){
print2("Performing final CNN normalization...")
m <- ModelList$SpatialTransformerSupp$FinalCNNBN$BN(m,
state = StateList$BNState_ImRep_FinalCNNBN,
inference = inference)
StateList$BNState_ImRep_FinalCNNBN <- m[[2]]
m <- m[[1]] * ModelList$SpatialTransformerSupp$FinalCNNBN$BNRescaler
}
}
}
print2("Returning ImageRepArm_SpatialArm outputs...")
return( list(m, StateList)  )
}
#ImageRepArm_batch <-                (ImageRepArm_batch_R <- function(ModelList, m, StateList, seed, MPList, inference){ print("DEBUG MODE IS ON IN IMAGE BACKBONE"); Sys.sleep(5L);
ImageRepArm_batch <- eq$filter_jit(ImageRepArm_batch_R <- function(ModelList, m, StateList, seed, MPList, inference){
ModelList <- MPList[[1]]$cast_to_compute( ModelList )
StateList <- MPList[[1]]$cast_to_compute( StateList )
# squeeze temporal dim if needed
thisPath <- T; if(!is.null(pretrainedModel)){ thisPath <- !grepl(pretrainedModel,pattern="video") }
if(thisPath){
if(dataType == "video" & is.null(pretrainedModel)){ m <- jnp$reshape(m, c(-1L, (orig_shape_m <- jnp$shape(m))[3:5])) }
print2(sprintf("Image stack dims: [%s]", paste(unlist(m$shape),collapse=",")))
# get spatial representation if not pure application of pre-trained
if( is.null(pretrainedModel) | grepl(pretrainedModel,pattern="-ft") ){
m <- jax$vmap(function(ModelList, m,
StateList, seed, MPList, inference){
ImageRepArm_SpatialArm(ModelList, m,
StateList, seed, MPList, inference) },
in_axes = list(NULL, 0L, NULL, NULL, NULL, NULL),
axis_name = batch_axis_name,
out_axes = list(0L,NULL))(ModelList, m, StateList, seed, MPList, inference)
StateList <- m[[2]]; m <- m[[1]]
}
# unsqueeze temporal dim if needed
if(dataType == "video" | NeuralizeScale){
if( is.null(pretrainedModel) ){ m <- jnp$reshape(m, c(orig_shape_m[1:2], -1L)) }
# (np$array(m)[1:5,,sample(1:10,1)]); m$shape
m <- jax$vmap(function(ModelList, m,
StateList, seed, MPList, inference){
TransformerBackbone(ModelList, m,
StateList, seed, MPList, inference, type = "Temporal")},
in_axes = list(NULL, 0L, NULL, NULL, NULL, NULL),
axis_name = batch_axis_name,
out_axes = list(0L,NULL))(ModelList, m,
StateList, jnp$add(seed,42L), MPList, inference)
StateList <- m[[2]]; m <- m[[1]]
print2("Returning ImageRepArm_TemporalArm outputs...")
}
}
print2("Returning ImageRepArm_batch outputs...")
return( list(m, StateList) )
})
}
Representations <- NULL; if(getRepresentations){
Representations <- matrix(NA,nrow = length(unique(imageKeysOfUnits)),
ncol = ifelse(optimizeImageRep, yes = nWidth_ImageRep,
no = nWidth_ImageRep+2L*nWidth_ImageRep*(imageModelClass=="CNN")*is.null(pretrainedModel) ))
usedImageKeys <- c(); last_i <- 0; ok_counter <- 0; ok<-F; while(!ok){
ok_counter <- ok_counter + 1
batch_indices <- (last_i+1):(last_i+batchSize)
batch_indices <- batch_indices[batch_indices <= length(unique(imageKeysOfUnits))]
last_i <- batch_indices[ length(batch_indices) ]
# checks for last / batch size corrections
if(last_i == length(unique(imageKeysOfUnits))){ ok <- T }
batchSizeOneCorrection <- ifelse(length(batch_indices) == 1, yes = TRUE, no = FALSE)
# get the data
setwd(orig_wd); batch_inference <- GetElementFromTfRecordAtIndices( uniqueKeyIndices = batch_indices,
filename = file,
nObs = length(unique(imageKeysOfUnits)),
return_iterator = T,
readVideo = useVideo,
image_dtype = image_dtype_tf,
iterator = ifelse(ok_counter > 1,
yes = list(saved_iterator),
no = list(NULL))[[1]] ); setwd(new_wd)
if(batchSizeOneCorrection){
batch_indices <- c(batch_indices,batch_indices)
batch_inference[[1]][[1]] <- tf$concat(list(tf$expand_dims(batch_inference[[1]][[1]],0L),
tf$expand_dims(batch_inference[[1]][[1]],0L)), 0L)
}
saved_iterator <- batch_inference[[2]]
batch_inference <- batch_inference[[1]]
batch_keys <- unlist(  lapply( p2l(batch_inference[[3]]$numpy()), as.character) )
gc(); py_gc$collect() # collect memory
# im <- jnp$array(batch_inference[[1]]); seed <- jax$random$PRNGKey(ai(2L+ok_counter + seed)); inference = T
# plot( np$array( jnp$array(batch_inference[[1]]))[,,1,3])# check variability across units
# batch_inference[[1]][3,,,1] # check image inputs
representation_ <-  np$array( ImageRepArm_batch(ModelList,
InitImageProcess(jnp$array(batch_inference[[1]]),
jax$random$PRNGKey(ai(2L+ok_counter + seed)), inference = T),
StateList,
jax$random$PRNGKey(ai(last_i + seed)),
MPList,
T # inference
)[[1]]  )
# plot(representation_[,sample(1:20)]); hist(as.matrix(representation_)); apply(as.matrix(representation_),2,sd)
if(T == F){
# sanity checks
tmp <- np$array(jnp$take(jnp$array(batch_inference[[1]]),(iCheck <- 8L)-1L,axis=0L))
image2(tmp[,,1])
cbind(lat[which(imageKeysOfUnits %in% batch_keys[ iCheck ])], long[which(imageKeysOfUnits %in% batch_keys[ iCheck ])])
}
if(any(is.na(representation_)) &  !initializingFxns){
stop("Stopping due to missingness in intermediary representation_ [Code reference: ImageModelBackbone.R]")
}
if("try-error" %in% class(representation_)){
print(representation_)
stop("Stopping due to try-error in *intermediary* version of representation_ [Code reference: ImageModelBackbone.R]")
}
if(batchSizeOneCorrection){ representation_ <- representation_[1,] }
usedImageKeys <- c(usedImageKeys, batch_keys)
Representations[batch_indices,] <- representation_
print2(sprintf("%.2f%% done getting image/video representations", 100*last_i / length(unique(imageKeysOfUnits))))
}
Representations <- Representations[match(as.character(imageKeysOfUnits), as.character(usedImageKeys) ),]
print2(sprintf("Done getting image/video representations!"))
}
# reset wd (may have been changed via tfrecords use)
setwd(  orig_wd  )
print2("Obtaining approximate parameter count...")
nParamsRep <- sum(unlist(lapply(jax$tree_leaves(eq$partition(ModelList, eq$is_array)[[1]]), function(zer){zer$size})))
if(is.null(pretrainedModel) & optimizeImageRep == F){ nParamsRep <- nParamsRep }
if(!is.null(pretrainedModel) ){
if(dataType == "image"){ nParamsRep <- nParameters_Pretrained }
if(dataType == "video" & !grepl(pretrainedModel,pattern="video")){ nParamsRep <- nParamsRep + nParameters_Pretrained }
if(dataType == "video" & grepl(pretrainedModel,pattern="video")){ nParamsRep <- nParameters_Pretrained }
}
# sanity check
if(any(is.na(Representations)) & !initializingFxns){
stop("Stopping due to missingness in *output* version of Representations [Code reference: ImageModelBackbone.R]")
}
if(CleanupEnv){
suppressWarnings( rm(PretrainedImageModelName, FeatureExtractor, ClayModel, TransformersModel, nParameters_Pretrained, pos = .GlobalEnv) )
}
if(returnContents){
return( list( "ImageRepresentations"= Representations,
"ImageRepArm_batch_R" = ImageRepArm_batch_R,
"ImageRepArm_batch" = ImageRepArm_batch,
"ImageModel_And_State_And_MPPolicy_List" = list(ModelList, StateList, MPList),
"InitImageProcess" = InitImageProcess,
"nParamsRep" = nParamsRep
) )
}
suppressWarnings(rm(ModelList, StateList, MPList)); gc()
}
# obtain image representation (random neural projection)
MyImageEmbeddings_RandomProj <- causalimages::GetImageRepresentations(
file  = TfRecord_name,
imageKeysOfUnits = KeysOfObservations[ take_indices ],
nDepth_ImageRep = 1L,
nWidth_ImageRep = 128L,
CleanupEnv = T
)
# obtain image representation (random neural projection)
MyImageEmbeddings_RandomProj <- causalimages::GetImageRepresentations(
file  = TfRecord_name,
imageKeysOfUnits = KeysOfObservations[ take_indices ],
nDepth_ImageRep = 1L,
nWidth_ImageRep = 128L,
CleanupEnv = T
)
#!/usr/bin/env Rscript
{
################################
# Image and image-sequence embeddings tutorial using causalimages
################################
# start with a clean environment
rm(list=ls()); options(error = NULL)
# remote install latest version of the package if needed
# devtools::install_github(repo = "cjerzak/causalimages-software/causalimages")
# local install for development team
# install.packages("~/Documents/causalimages-software/causalimages",repos = NULL, type = "source",force = F)
# build backend you haven't ready:
# causalimages::BuildBackend()
# load in package
library( causalimages  )
library(reticulate)
# load in tutorial data
data(  CausalImagesTutorialData )
# example acquire image function (loading from memory)
# in general, you'll want to write a function that returns images
# that saved disk associated with keys
acquireImageFromMemory <- function(keys){
# here, the function input keys
# refers to the unit-associated image keys
m_ <- FullImageArray[match(keys, KeysOfImages),,,]
# if keys == 1, add the batch dimension so output dims are always consistent
# (here in image case, dims are batch by height by width by channel)
if(length(keys) == 1){
m_ <- array(m_,dim = c(1L,dim(m_)[1],dim(m_)[2],dim(m_)[3]))
}
return( m_ )
}
# drop first column
X <- X[,-1]
# mean imputation for simplicity
X <- apply(X,2,function(zer){
zer[is.na(zer)] <- mean( zer,na.rm = T )
return( zer )
})
# select observation subset to make tutorial analyses run fast
take_indices <- unlist( tapply(1:length(obsW),obsW,function(zer){ sample(zer, 50) }) )
# write tf record
TfRecord_name <- "/Users/jspgr33n/Desktop/QTM-495/tutorials/CausalImagesTutorialData.tfrecord"
causalimages::WriteTfRecord(  file =  TfRecord_name,
uniqueImageKeys = unique( KeysOfObservations[ take_indices ] ),
acquireImageFxn = acquireImageFromMemory  )
# obtain image representation (random neural projection)
MyImageEmbeddings_RandomProj <- causalimages::GetImageRepresentations(
file  = TfRecord_name,
imageKeysOfUnits = KeysOfObservations[ take_indices ],
nDepth_ImageRep = 1L,
nWidth_ImageRep = 128L,
CleanupEnv = T
)
# sanity check - # of rows in MyImageEmbeddings matches # of image keys
nrow(MyImageEmbeddings_RandomProj$ImageRepresentations) == length(KeysOfObservations[ take_indices ])
# each row in MyImageEmbeddings$ImageRepresentations corresponds to an observation
# each column represents an embedding dimension associated with the imagery for that location
plot( MyImageEmbeddings_RandomProj$ImageRepresentations  )
# obtain image representation (pre-trained ViT)
MyImageEmbeddings_ViT <- causalimages::GetImageRepresentations(
file  = TfRecord_name,
pretrainedModel = "vit-base",
imageKeysOfUnits = KeysOfObservations[ take_indices ]#,CleanupEnv = T
)
# analyze ViT representations
plot( MyImageEmbeddings_ViT$ImageRepresentations  )
# obtain image representation (pre-trained CLIP-RCSID)
MyImageEmbeddings_Clip <- causalimages::GetImageRepresentations(
file  = TfRecord_name,
pretrainedModel = "clip-rsicd",
imageKeysOfUnits = KeysOfObservations[ take_indices ],
CleanupEnv = T)
# analyze Clip representations
plot( MyImageEmbeddings_Clip$ImageRepresentations  )
# sanity check - # of rows in MyImageEmbeddings matches # of image keys
nrow(MyImageEmbeddings_Clip$ImageRepresentations) == length(KeysOfObservations[ take_indices ])
# other output quantities include the image model functions and model parameters
names(  MyImageEmbeddings_Clip  )[-1]
print("Done with image representations tutorial!")
}
reticulate::py_last_error()
